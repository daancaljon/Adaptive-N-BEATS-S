{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import plotly.graph_objects as go\n",
    "import random\n",
    "import os\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from time import time\n",
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#from pytorchtools import EarlyStopping\n",
    "#%pip install wandb --upgrade\n",
    "#!pip install wandb\n",
    "import wandb\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0., device='cuda:0')\n",
      "1.13.1\n"
     ]
    }
   ],
   "source": [
    "a = torch.zeros(5).cuda()\n",
    "print(a[4])\n",
    "print(torch.__version__)\n",
    "\n",
    "os.environ[\"WANDB_START_METHOD\"] = \"thread\"\n",
    "del(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading & data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load monthly M4 data.\n",
    "# Transform the data into a lists of arrays. Each inner array represents a timeseries.\n",
    "# Remove all the NaN values from the datasets.\n",
    "\n",
    "# M4\n",
    "trainset = pd.read_csv('https://raw.githubusercontent.com/Mcompetitions/M4-methods/master/Dataset/Train/Monthly-train.csv')\n",
    "testset = pd.read_csv('https://raw.githubusercontent.com/Mcompetitions/M4-methods/master/Dataset/Test/Monthly-test.csv')\n",
    "trainset.set_index('V1', inplace = True)\n",
    "testset.set_index('V1', inplace = True)\n",
    "# Add the testset columns behind the trainset columns\n",
    "testset_merge = trainset.merge(testset, on = 'V1', how = 'inner')\n",
    "# Get the data in numpy representation\n",
    "trainset_np = trainset.values\n",
    "testset_np = testset_merge.values\n",
    "# Select all non NaN values from the trainset\n",
    "trainset_clean = [x[x == x] for x in trainset_np]\n",
    "# Train/validation/test --------------------------------- NBeats paper validation strategy\n",
    "testset_m4m = [x[x == x] for x in testset_np]\n",
    "valset_m4m = trainset_clean.copy()\n",
    "trainset_m4m = [x[:-18] for x in trainset_clean]\n",
    "\n",
    "\n",
    "del(trainset, testset, testset_merge, trainset_np, testset_np, trainset_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## M3 (you need to have the file on your device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"M3_monthly_TSTS.csv\")\n",
    "\n",
    "mydata = list()\n",
    "temp_list = list()\n",
    "my_id = \"M1\" #first ID\n",
    "for num, row in df.iterrows():\n",
    "    if my_id == row[\"series_id\"]:\n",
    "        temp_list.append(row[\"value\"])\n",
    "    else:\n",
    "        my_id = row[\"series_id\"]\n",
    "        mydata.append(np.array(temp_list))\n",
    "        temp_list = list()\n",
    "        temp_list.append(row[\"value\"])\n",
    "        \n",
    "mydata.append(np.array(temp_list)) #laatste nog toevoegen\n",
    "dataset_clean = [x[x == x] for x in mydata]        \n",
    "\n",
    "testset_m4m = dataset_clean.copy() \n",
    "valset_m4m = [x[:-18] for x in testset_m4m] #IN EVALUATION EN TESTING OOK VERANDEREN!!!!\n",
    "trainset_m4m = [x[:-18] for x in valset_m4m]\n",
    "\n",
    "#trainset_np, testset_np = train_test_split(mydata, test_size = 0.25, random_state = 2000)\n",
    "#print(len(trainset_np), len(testset_np))\n",
    "#testset_m4m = [x[x == x] for x in testset_np]\n",
    "#valset_m4m = trainset_np.copy()\n",
    "#trainset_m4m = [x[:-18] for x in trainset_np]\n",
    "#print(len(valset_m4m[1]),len(trainset_m4m[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2640. 2640. 2160. 4200. 3360. 2400. 3600. 1920. 4200. 4560.  480. 3720.\n",
      " 5640. 2880. 1800. 3120. 2400. 2520. 9000. 2640. 3120. 2880. 8760. 5160.\n",
      " 2160. 8280. 4920. 3120. 6600. 4080. 5880. 1680. 6720. 2040. 6480. 1920.\n",
      " 3600. 2040. 2760. 3840.  960. 2280. 1320. 2160. 4800. 3000. 3120. 5880.\n",
      " 2640. 2400. 2280.  480. 5040. 1920.  840. 2520. 1560. 1440.  240. 1800.\n",
      " 4680. 1800. 1680. 3720. 2160.  480. 2040. 1440.]\n",
      "[1680. 1920.  120. 1080.  840. 1440.  480.  720. 4080. 1560.  480.  720.\n",
      " 6120. 2040. 3960. 2160.  120. 1200. 1080. 1080. 1080. 2160.  240. 1440.\n",
      " 1200. 1560. 2520.  600. 1560. 3240. 7440.  480. 2640.  960. 3120. 1200.\n",
      "  960.  480.  600.  120. 2640.  720.  600.  840. 1320. 2160. 1200. 1800.\n",
      " 1320.  600.]\n",
      "[2622.4 2607.5 2556.6 2569.3 2533.2 2529.  2577.8 2556.6 2558.7 2541.7\n",
      " 2473.8 2461.  2435.5 2414.3 2350.6 2329.4 2278.4 2252.9 2269.9 2227.4\n",
      " 2195.6 2204.1 2195.6 2202.  2157.4 2142.5 2125.5 2110.7 2072.4 2076.7\n",
      " 2095.8 2023.6 2004.5 1985.4 1953.5]\n",
      "1428\n"
     ]
    }
   ],
   "source": [
    "print(testset_m4m[0]) \n",
    "print(valset_m4m[1])\n",
    "print(trainset_m4m[1427])\n",
    "print(len(trainset_m4m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "del(df, temp_list,mydata,dataset_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scale independent loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output = batch x fl\n",
    "# target = batch x fl\n",
    "# actuals_train = batch x bl\n",
    "\n",
    "def SMAPE(output, target, actuals_train = None):\n",
    "    \n",
    "    abs_errors = torch.abs(target - output)\n",
    "    abs_output = torch.abs(output)\n",
    "    abs_target = torch.abs(target)\n",
    "    loss = 200 * torch.mean(abs_errors / (abs_output.detach() + abs_target + 1e-5))\n",
    "    # possibly nan values in training networks if no offset in denominator is used\n",
    "    \n",
    "    return loss\n",
    "\n",
    "\n",
    "def MAPE(output, target, actuals_train = None):\n",
    "\n",
    "    abs_errors = torch.abs(target - output)\n",
    "    abs_target = torch.abs(target)\n",
    "    loss = 100 * torch.mean(abs_errors / (abs_target + 1e-5))\n",
    "    # possibly nan values in training networks if no offset in denominator is used\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "def MASE(output, target, actuals_train):\n",
    "    \n",
    "    mask = torch.abs(actuals_train)>1e-6\n",
    "    mad = torch.sum(torch.abs(actuals_train[:, 1:] - actuals_train[:, :-1]), dim = -1) / (torch.sum(mask, dim = -1) - 1)\n",
    "    mad_reshaped = mad.unsqueeze(-1).repeat_interleave(target.shape[-1], dim = -1)\n",
    "    loss_items = torch.mean((torch.abs(target - output)) / (mad_reshaped + 1e-5), dim = -1)\n",
    "    loss_items_clamped = torch.clamp(loss_items, 0, 5)\n",
    "    loss = torch.mean(loss_items_clamped)\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "def MASE_m(output, target, actuals_train):\n",
    "    \n",
    "    mask = torch.abs(actuals_train)>1e-6\n",
    "    mad = torch.sum(torch.abs(actuals_train[:, 12:] - actuals_train[:, :-12]), dim = -1) / (torch.sum(mask, dim = -1) - 12)\n",
    "    mad_reshaped = mad.unsqueeze(-1).repeat_interleave(target.shape[-1], dim = -1)\n",
    "    loss_items = torch.mean((torch.abs(target - output)) / (mad_reshaped + 1e-5), dim = -1)\n",
    "    loss_items_clamped = torch.clamp(loss_items, 0, 5)\n",
    "    loss = torch.mean(loss_items_clamped) \n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "def RMSSE(output, target, actuals_train):\n",
    "    \n",
    "    mask = torch.abs(actuals_train)>1e-6\n",
    "    msd = torch.sum((actuals_train[:, 1:] - actuals_train[:, :-1])**2, dim = -1) / (torch.sum(mask, dim = -1) - 1)\n",
    "    msd_reshaped = msd.unsqueeze(-1).repeat_interleave(target.shape[-1], dim = -1)\n",
    "    loss_items = torch.sqrt(torch.mean((target - output)**2 / (msd_reshaped + 1e-5), dim = -1))\n",
    "    loss_items_clamped = torch.clamp(loss_items, 0, 5)\n",
    "    loss = torch.mean(loss_items_clamped)\n",
    "    #loss = torch.sqrt(torch.mean((target - output)**2 / msd_reshaped))\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "def RMSSE_m(output, target, actuals_train):\n",
    "    \n",
    "    mask = torch.abs(actuals_train)>1e-6\n",
    "    msd = torch.sum((actuals_train[:, 12:] - actuals_train[:, :-12])**2, dim = -1) / (torch.sum(mask, dim = -1) - 12)\n",
    "    msd_reshaped = msd.unsqueeze(-1).repeat_interleave(target.shape[-1], dim = -1)\n",
    "    loss_items = torch.sqrt(torch.mean((target - output)**2 / (msd_reshaped + 1e-5), dim = -1))\n",
    "    loss_items_clamped = torch.clamp(loss_items, 0, 5)\n",
    "    loss = torch.mean(loss_items_clamped)\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NBeats models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generic building block:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenericNBeatsBlock(nn.Module):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 device,\n",
    "                 backcast_length,\n",
    "                 forecast_length,\n",
    "                 hidden_layer_units, thetas_dims, \n",
    "                 share_thetas,\n",
    "                 dropout = False, dropout_p = 0.0, \n",
    "                 neg_slope = 0.00):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.backcast_length = backcast_length\n",
    "        self.forecast_length = forecast_length        \n",
    "        if isinstance(hidden_layer_units, int):\n",
    "            self.hidden_layer_units = [hidden_layer_units for FC_layer in range(4)]\n",
    "        else:\n",
    "            #assert(len(hidden_layer_units) == 4)\n",
    "            self.hidden_layer_units = hidden_layer_units\n",
    "        self.thetas_dims = thetas_dims\n",
    "        self.share_thetas = share_thetas\n",
    "        self.dropout = dropout\n",
    "        self.dropout_p = dropout_p\n",
    "        self.neg_slope = neg_slope\n",
    "        \n",
    "        # shared layers in block\n",
    "        self.fc1 = nn.Linear(self.backcast_length,\n",
    "                             self.hidden_layer_units[0])#, bias = False)\n",
    "        self.fc2 = nn.Linear(self.hidden_layer_units[0], self.hidden_layer_units[1])#, bias = False)\n",
    "        self.fc3 = nn.Linear(self.hidden_layer_units[1], self.hidden_layer_units[2])#, bias = False)\n",
    "        self.fc4 = nn.Linear(self.hidden_layer_units[2], self.hidden_layer_units[3])#, bias = False)\n",
    "        \n",
    "        # do not use F.dropout as you want dropout to only affect training (not evaluation mode)\n",
    "        # nn.Dropout handles this automatically\n",
    "        if self.dropout:\n",
    "            self.dropoutlayer = nn.Dropout(p = self.dropout_p)\n",
    "        \n",
    "        # task specific (backcast & forecast) layers in block\n",
    "        # do not include bias - see section 3.1 - Ruben does include bias for generic blocks\n",
    "        if self.share_thetas:\n",
    "            self.theta_b_fc = self.theta_f_fc = nn.Linear(self.hidden_layer_units[3], self.thetas_dims)#, bias = False)\n",
    "        else:\n",
    "            self.theta_b_fc = nn.Linear(self.hidden_layer_units[3], self.thetas_dims)#, bias = False)\n",
    "            self.theta_f_fc = nn.Linear(self.hidden_layer_units[3], self.thetas_dims)#, bias = False)\n",
    "        \n",
    "        # block output layers\n",
    "        self.backcast_out = nn.Linear(self.thetas_dims, self.backcast_length)#, bias = False) # include bias - see section 3.3\n",
    "        self.forecast_out = nn.Linear(self.thetas_dims, self.forecast_length)#, bias = False) # include bias - see section 3.3\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        if self.dropout:\n",
    "            h1 = F.leaky_relu(self.fc1(x.to(self.device)), negative_slope = self.neg_slope)\n",
    "            h1 = self.dropoutlayer(h1)\n",
    "            h2 = F.leaky_relu(self.fc2(h1), negative_slope = self.neg_slope)\n",
    "            h2 = self.dropoutlayer(h2)\n",
    "            h3 = F.leaky_relu(self.fc3(h2), negative_slope = self.neg_slope)\n",
    "            h3 = self.dropoutlayer(h3)\n",
    "            h4 = F.leaky_relu(self.fc4(h3), negative_slope = self.neg_slope)\n",
    "            theta_b = F.leaky_relu(self.theta_b_fc(h4), negative_slope = self.neg_slope)\n",
    "            #theta_b = self.theta_b_fc(h4)\n",
    "            theta_f = F.leaky_relu(self.theta_f_fc(h4), negative_slope = self.neg_slope)\n",
    "            #theta_f = self.theta_f_fc(h4)\n",
    "            backcast = self.backcast_out(theta_b)\n",
    "            forecast = self.forecast_out(theta_f)\n",
    "        else:\n",
    "            h1 = F.leaky_relu(self.fc1(x.to(self.device)), negative_slope = self.neg_slope)\n",
    "            h2 = F.leaky_relu(self.fc2(h1), negative_slope = self.neg_slope)\n",
    "            h3 = F.leaky_relu(self.fc3(h2), negative_slope = self.neg_slope)\n",
    "            h4 = F.leaky_relu(self.fc4(h3), negative_slope = self.neg_slope)\n",
    "            theta_b = F.leaky_relu(self.theta_b_fc(h4), negative_slope = self.neg_slope)\n",
    "            #theta_b = self.theta_b_fc(h4)\n",
    "            theta_f = F.leaky_relu(self.theta_f_fc(h4), negative_slope = self.neg_slope)\n",
    "            #theta_f = self.theta_f_fc(h4)\n",
    "            backcast = self.backcast_out(theta_b)\n",
    "            forecast = self.forecast_out(theta_f)\n",
    "            \n",
    "        return backcast, forecast\n",
    "    \n",
    "    \n",
    "    def __str__(self):\n",
    "        \n",
    "        block_type = type(self).__name__\n",
    "        \n",
    "        return f'{block_type}(units={self.hidden_layer_units}, thetas_dims={self.thetas_dims}, ' \\\n",
    "            f'backcast_length={self.backcast_length}, ' \\\n",
    "            f'forecast_length={self.forecast_length}, share_thetas={self.share_thetas}, ' \\\n",
    "            f'dropout={self.dropout}, dropout_p={self.dropout_p}, neg_slope={self.neg_slope}) at @{id(self)}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "StableNBeatsNet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only the forward method is changed compared to standard NBeatsNet\n",
    "class StableNBeatsNet(nn.Module): \n",
    "    \n",
    "    def __init__(self, \n",
    "                 device,\n",
    "                 backcast_length_multiplier,\n",
    "                 forecast_length,\n",
    "                 hidden_layer_units, thetas_dims, \n",
    "                 share_thetas,\n",
    "                 nb_blocks_per_stack, n_stacks, share_weights_in_stack,\n",
    "                 dropout = False, dropout_p = 0.0, \n",
    "                 neg_slope = 0.00):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.backcast_length = backcast_length_multiplier * forecast_length\n",
    "        self.forecast_length = forecast_length\n",
    "        self.hidden_layer_units = hidden_layer_units\n",
    "        self.thetas_dims = thetas_dims\n",
    "        self.share_thetas = share_thetas\n",
    "        self.nb_blocks_per_stack = nb_blocks_per_stack\n",
    "        self.n_stacks = n_stacks\n",
    "        self.share_weights_in_stack = share_weights_in_stack\n",
    "        self.dropout = dropout\n",
    "        self.dropout_p = dropout_p\n",
    "        self.neg_slope = neg_slope\n",
    "        \n",
    "        self.stacks = []\n",
    "        self.parameters = []\n",
    "        \n",
    "        print(f'| N-Beats')\n",
    "        for stack_id in range(self.n_stacks):\n",
    "            self.stacks.append(self.create_stack(stack_id))\n",
    "        self.parameters = nn.ParameterList(self.parameters)\n",
    "        \n",
    "        \n",
    "    def create_stack(self, stack_id):\n",
    "        \n",
    "        print(f'| --  Stack Generic (#{stack_id}) (share_weights_in_stack={self.share_weights_in_stack})')\n",
    "        blocks = []\n",
    "        for block_id in range(self.nb_blocks_per_stack):\n",
    "            if self.share_weights_in_stack and block_id != 0:\n",
    "                block = blocks[-1]  # pick up the last one when we share weights\n",
    "            else:\n",
    "                block = GenericNBeatsBlock(self.device,\n",
    "                                           self.backcast_length,\n",
    "                                           self.forecast_length,\n",
    "                                           self.hidden_layer_units, self.thetas_dims, \n",
    "                                           self.share_thetas,\n",
    "                                           self.dropout, self.dropout_p, \n",
    "                                           self.neg_slope)\n",
    "                self.parameters.extend(block.parameters())\n",
    "                print(f'     | -- {block}')\n",
    "                blocks.append(block)\n",
    "                \n",
    "        return blocks\n",
    "\n",
    "    \n",
    "    def forward(self, backcast_arr):\n",
    "        \n",
    "        # dim backcast_arr = batch_size x shifts x backcast_length\n",
    "        # shifts == 0 is standard input window, others are shifted lookback windows \n",
    "        # higher index = further back in time\n",
    "        # feed different input windows (per batch) through the SAME network (check via list of learnable parameters)\n",
    "        # see https://stackoverflow.com/questions/54444630/application-of-nn-linear-layer-in-pytorch-on-additional-dimentions\n",
    "        \n",
    "        forecast_arr = torch.zeros((backcast_arr.shape[0], # take batch size from backcast\n",
    "                                    backcast_arr.shape[1], # take n of shifts from backcast\n",
    "                                    self.forecast_length), dtype = torch.float).to(self.device)\n",
    "        backcast_arr = backcast_arr.to(self.device)\n",
    "        \n",
    "        # loop through stacks (and blocks)\n",
    "        for stack_id in range(len(self.stacks)):\n",
    "            for block_id in range(len(self.stacks[stack_id])):\n",
    "                b, f = self.stacks[stack_id][block_id](backcast_arr)\n",
    "                backcast_arr = backcast_arr - b\n",
    "                forecast_arr = forecast_arr + f  \n",
    "                \n",
    "        return backcast_arr, forecast_arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Training and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_torch(seed = 5101992):\n",
    "    \n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StableNBeatsLearner:\n",
    "    \n",
    "    def __init__(self,\n",
    "                 device,\n",
    "                 forecast_length,\n",
    "                 configNBeats):\n",
    "        \n",
    "        gc.collect()\n",
    "                \n",
    "        self.device = device \n",
    "        self.forecast_length = forecast_length\n",
    "        self.configNBeats = configNBeats\n",
    "        \n",
    "        if self.configNBeats[\"loss_function\"] == 1:\n",
    "            self.loss = RMSSE\n",
    "        elif self.configNBeats[\"loss_function\"] == 2:\n",
    "            self.loss = RMSSE_m\n",
    "        elif self.configNBeats[\"loss_function\"] == 3:\n",
    "            self.loss = SMAPE\n",
    "        elif self.configNBeats[\"loss_function\"] == 4:\n",
    "            self.loss = MAPE\n",
    "            \n",
    "        self.rndseed = self.configNBeats[\"rndseed\"]\n",
    "        seed_torch(self.rndseed)\n",
    "        \n",
    "        print('--- Model ---')    \n",
    "        self.model = StableNBeatsNet(self.device,\n",
    "                                     self.configNBeats[\"backcast_length_multiplier\"],\n",
    "                                     self.forecast_length,\n",
    "                                     self.configNBeats[\"hidden_layer_units\"],\n",
    "                                     self.configNBeats[\"thetas_dims\"],\n",
    "                                     self.configNBeats[\"share_thetas\"],\n",
    "                                     self.configNBeats[\"nb_blocks_per_stack\"],\n",
    "                                     self.configNBeats[\"n_stacks\"],\n",
    "                                     self.configNBeats[\"share_weights_in_stack\"],                               \n",
    "                                     self.configNBeats[\"dropout\"],\n",
    "                                     self.configNBeats[\"dropout_p\"],\n",
    "                                     self.configNBeats[\"neg_slope\"])\n",
    "        \n",
    "        self.model = self.model.to(self.device)\n",
    "        \n",
    "        self.optim = torch.optim.Adam(self.model.parameters(), \n",
    "                                      lr = self.configNBeats[\"learning_rate\"],\n",
    "                                      weight_decay = self.configNBeats[\"weight_decay\"])\n",
    "        \n",
    "        self.init_state = copy.deepcopy(self.model.state_dict())\n",
    "        self.init_state_opt = copy.deepcopy(self.optim.state_dict())\n",
    "        \n",
    "        wandb.watch(self.model)\n",
    "    \n",
    "    \n",
    "    def ts_padding(self, ts_train_data, ts_eval_data):\n",
    "        \n",
    "        # Some time series in the dataset are not long enough to support the specified:\n",
    "        # forecast_length and backcast_length + backcast input shifts\n",
    "        # we use zero padding for the time series that are too short (neutral effect on loss calculations)\n",
    "        # + self.shifts comes from the number of extra observations needed to create the shifted inputs/targets\n",
    "        # + (self.forgins - 1) comes from rolling origin evaluation\n",
    "        \n",
    "        length_train = (self.configNBeats[\"backcast_length_multiplier\"] * self.forecast_length + \n",
    "                        self.forecast_length +\n",
    "                        self.shifts)\n",
    "        length_eval = (self.configNBeats[\"backcast_length_multiplier\"] * self.forecast_length +\n",
    "                       self.forecast_length +\n",
    "                       self.shifts +\n",
    "                       (self.forigins - 1))\n",
    "        \n",
    "        ts_train_pad = [x if x.size >= length_train else np.pad(x,\n",
    "                                                                (int(length_train - x.size), 0), \n",
    "                                                                'constant', \n",
    "                                                                constant_values = 0) for x in ts_train_data]\n",
    "        ts_eval_pad = [x if x.size >= length_eval else np.pad(x,\n",
    "                                                              (int(length_eval - x.size), 0), \n",
    "                                                              'constant', \n",
    "                                                              constant_values = 0) for x in ts_eval_data]\n",
    "        \n",
    "        return ts_train_pad, ts_eval_pad\n",
    "        \n",
    "        \n",
    "    def make_batch(self, batch_data, shuffle_origin = True):\n",
    "        \n",
    "        # If shuffle_origin = True --> batch for training --> random forecast origin based on LH \n",
    "        # If shuffle_origin = False --> batch for evaluation --> fixed forecast origin\n",
    "        \n",
    "        # Split the batch into input_list and target_list\n",
    "        # In x_arr and target_arr: batch x shift x backcats_length/forecast_length\n",
    "        x_arr = np.empty(shape = (len(batch_data), \n",
    "                                  self.shifts + 1,\n",
    "                                  self.configNBeats[\"backcast_length_multiplier\"] * self.forecast_length))\n",
    "        target_arr = np.empty(shape = (len(batch_data), \n",
    "                                       self.shifts + 1, \n",
    "                                       self.forecast_length))\n",
    "        \n",
    "        # For every time series in the batch:\n",
    "        # (1) slice the time series according to specific forecasting origin (depending on shuffle_origin)\n",
    "        # (2) make shifted inputs/targets --> max number of shifts = forecast_length - 1\n",
    "        # (3) fill x_arr and target_arr\n",
    "        for j in range(len(batch_data)):\n",
    "            i = batch_data[j]\n",
    "            \n",
    "            if shuffle_origin: \n",
    "                # suffle_origin --> only in training \n",
    "                \n",
    "                ### --> also pick random scale --> does not result in improved results\n",
    "                ### to remain as close as possible to nbeats paper: do not pick random scale\n",
    "                ### i = i + i * np.random.default_rng().uniform(-0.95, 0.95, 1)\n",
    "                \n",
    "                # pick origin\n",
    "                LH_max_offset = int(self.configNBeats[\"LH\"] * self.forecast_length)\n",
    "                ts_max_offset = int(len(i) -\n",
    "                                    (self.configNBeats[\"backcast_length_multiplier\"] * self.forecast_length + \n",
    "                                     self.forecast_length +\n",
    "                                     self.shifts))\n",
    "                max_offset = min(LH_max_offset, ts_max_offset)\n",
    "                if max_offset < 1:\n",
    "                    offset = np.zeros(1)\n",
    "                else:\n",
    "                    offset = np.random.randint(low = 0, high = max_offset)   \n",
    "            else:\n",
    "                offset = np.zeros(1)\n",
    "            \n",
    "            if offset == 0:\n",
    "                for shift in range(self.shifts + 1):\n",
    "                    if shift == 0:\n",
    "                        x_arr[j, shift, :] = i[-self.forecast_length-self.configNBeats[\"backcast_length_multiplier\"]*self.forecast_length:-self.forecast_length]\n",
    "                        target_arr[j, shift, :] = i[-self.forecast_length:]\n",
    "                    else:\n",
    "                        x_arr[j, shift, :] = i[-self.forecast_length-self.configNBeats[\"backcast_length_multiplier\"]*self.forecast_length-shift:-self.forecast_length-shift]\n",
    "                        target_arr[j, shift, :] = i[-self.forecast_length-shift:-shift]\n",
    "            else:\n",
    "                for shift in range(self.shifts + 1):\n",
    "                    x_arr[j, shift, :] = i[-self.forecast_length-self.configNBeats[\"backcast_length_multiplier\"]*self.forecast_length-offset-shift:-self.forecast_length-offset-shift]\n",
    "                    target_arr[j, shift, :] = i[-self.forecast_length-offset-shift:-offset-shift]\n",
    "                    \n",
    "        return x_arr, target_arr\n",
    "                                        \n",
    "                    \n",
    "    def create_example_plots(self, output, target, actuals_train, final_evaluation = False):\n",
    "        \n",
    "        plot_forecasts = torch.cat((actuals_train, output))\n",
    "        plot_actuals = torch.cat((actuals_train, target))\n",
    "        random_sample_forecasts = plot_forecasts.squeeze()\n",
    "        random_sample_actuals = plot_actuals.squeeze()\n",
    "        x_axis = torch.arange(1, random_sample_forecasts.shape[0]+1)\n",
    "        fig = go.Figure()\n",
    "        fig.add_trace(go.Scatter(x = x_axis.numpy(), y = random_sample_forecasts.numpy(),\n",
    "                                 mode = 'lines+markers', name = 'forecasts'))\n",
    "        fig.add_trace(go.Scatter(x = x_axis.numpy(), y = random_sample_actuals.numpy(),\n",
    "                                 mode = 'lines+markers', name = 'actuals'))\n",
    "        \n",
    "        # We only visualize examples for last epoch\n",
    "        if not final_evaluation:\n",
    "            wandb.log({\"example_plots_evaluation\": fig})\n",
    "        else:\n",
    "            wandb.log({\"example_plots_final_evaluation\": fig})\n",
    "            \n",
    "            \n",
    "    def evaluate(self, x_arr, target_arr,\n",
    "                 epoch = None,\n",
    "                 need_grad = True,\n",
    "                 early_stop = False):\n",
    "        \n",
    "        losses = dict()\n",
    "        \n",
    "        # Inputs must be converted to np.array of Tensors (float)\n",
    "        x_arr = torch.from_numpy(x_arr).float().to(self.device)\n",
    "        target_arr = torch.from_numpy(target_arr).float().to(self.device)\n",
    "        \n",
    "        if need_grad:\n",
    "            self.model.train() \n",
    "            self.model.to(self.device)\n",
    "            _, forecast_arr = self.model(x_arr) \n",
    "            \n",
    "            losses_forecast_shifts = 0.0\n",
    "            for shift in range(self.shifts + 1):\n",
    "                losses_forecast_shifts += self.loss(forecast_arr[:, shift, :], \n",
    "                                                    target_arr[:, shift, :], \n",
    "                                                    x_arr[:, shift, :])\n",
    "            losses[\"forecast_accuracy\"] = losses_forecast_shifts / (self.shifts + 1)\n",
    "                        \n",
    "            if self.shifts > 0:\n",
    "                # dimensions = batch_size x shifted forecasts for stability computations\n",
    "                forecast_base_arr = torch.zeros((forecast_arr.shape[0],\n",
    "                                                 sum(range(self.forecast_length - self.shifts, self.forecast_length))),\n",
    "                                                dtype = torch.float).to(self.device)\n",
    "                forecast_shift_arr = torch.zeros((forecast_arr.shape[0],\n",
    "                                                  sum(range(self.forecast_length - self.shifts, self.forecast_length))),\n",
    "                                                 dtype = torch.float).to(self.device)\n",
    "                col = 0\n",
    "                for shift in range(1, self.shifts + 1):\n",
    "                    for horizon_m1 in range(self.forecast_length - shift):\n",
    "                        forecast_base_arr[:, col] = forecast_arr[:, 0, horizon_m1]\n",
    "                        forecast_shift_arr[:, col] = forecast_arr[:, shift, horizon_m1 + shift]\n",
    "                        col = col + 1\n",
    "                losses[\"forecast_stability\"] = self.loss(forecast_shift_arr, forecast_base_arr, x_arr[:, 0, :])\n",
    "            else:\n",
    "                losses[\"forecast_stability\"] = torch.zeros(1)\n",
    "                \n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                \n",
    "                self.model.eval()\n",
    "                self.model.to(self.device)\n",
    "                \n",
    "                _, forecast_arr = self.model(x_arr) #Dit duurt lang! (wat is het verschil met forward)\n",
    "                \n",
    "                losses_forecast_shifts = 0.0\n",
    "                for shift in range(self.shifts + 1):\n",
    "                    losses_forecast_shifts += self.loss(forecast_arr[:, shift, :], target_arr[:, shift, :], x_arr[:, shift, :])\n",
    "                losses[\"forecast_accuracy\"] = losses_forecast_shifts / (self.shifts + 1)\n",
    "                \n",
    "                if self.shifts > 0:\n",
    "                    forecast_base_arr = torch.zeros((forecast_arr.shape[0],\n",
    "                                                     sum(range(self.forecast_length - self.shifts, self.forecast_length))),\n",
    "                                                    dtype = torch.float).to(self.device)\n",
    "                    forecast_shift_arr = torch.zeros((forecast_arr.shape[0],\n",
    "                                                      sum(range(self.forecast_length - self.shifts, self.forecast_length))),\n",
    "                                                     dtype = torch.float).to(self.device)\n",
    "                    col = 0\n",
    "                    #print(\"begin forecast stability loop\")\n",
    "                    for shift in range(1, self.shifts + 1):\n",
    "                        for horizon_m1 in range(self.forecast_length - shift):\n",
    "                            forecast_base_arr[:, col] = forecast_arr[:, 0, horizon_m1]\n",
    "                            forecast_shift_arr[:, col] = forecast_arr[:, shift, horizon_m1 + shift]\n",
    "                            col = col + 1\n",
    "                    #print(\"forecast stability klaar\")\n",
    "                    losses[\"forecast_stability\"] = self.loss(forecast_base_arr, forecast_shift_arr, x_arr[:, 0, :])\n",
    "                else:\n",
    "                    losses[\"forecast_stability\"] = torch.zeros(1)\n",
    "                    \n",
    "                if not self.disable_plot:\n",
    "                    if early_stop:\n",
    "                        # Plot validation examples - of standard/unshifted input - for last epoch before break\n",
    "                        # This part of the evaluation function is only called after training has been forced to stop\n",
    "                        self.create_example_plots(forecast_arr[0, 0, :], target_arr[0, 0, :], x_arr[0, 0, :])\n",
    "                    else:\n",
    "                        # Plot validation examples - of standard/unshifted input - for last epoch\n",
    "                        # This part of the evaluation function is called after training has been completed\n",
    "                        if (epoch == self.configNBeats[\"epochs\"]):\n",
    "                            self.create_example_plots(forecast_arr[0, 0, :], target_arr[0, 0, :], x_arr[0, 0, :])\n",
    "        \n",
    "        return losses\n",
    "    \n",
    "    \n",
    "    # Training of net (training data can include validation data) + validation or testing\n",
    "    def train_net(self,\n",
    "                  ts_train_m4m,\n",
    "                  ts_eval_m4m,\n",
    "                  forigins,\n",
    "                  validation = True,\n",
    "                  validation_earlystop = False,\n",
    "                  disable_plot = True):\n",
    "        \n",
    "        self.forigins = forigins\n",
    "        self.shifts = self.configNBeats[\"shifts\"]\n",
    "        self.validation = validation\n",
    "        self.validation_earlystop = validation_earlystop\n",
    "        self.disable_plot = disable_plot\n",
    "        #assert self.shifts < self.forecast_length # max allowed number of shifts is forecast_length - 1\n",
    "        \n",
    "        # Data preprocessing depends on backcast_length_multiplier\n",
    "        ts_train_pad, ts_eval_pad = self.ts_padding(ts_train_m4m, ts_eval_m4m)\n",
    "        ts_train_pad = np.array(ts_train_pad, dtype = object)\n",
    "        ts_eval_pad = np.array(ts_eval_pad, dtype = object)\n",
    "        \n",
    "        print('--- Training ---')\n",
    "        \n",
    "        # Containers to save train/evaluation losses and parameters\n",
    "        tloss_combined, tloss_forecast_accuracy, tloss_forecast_stability = [], [], []\n",
    "        eloss_combined, eloss_forecast_accuracy, eloss_forecast_stability = [], [], []\n",
    "        #params = []\n",
    "        \n",
    "        # Main training loop\n",
    "        self.model.load_state_dict(self.init_state)\n",
    "        self.optim.load_state_dict(self.init_state_opt)\n",
    "            \n",
    "        seed_torch(self.rndseed)\n",
    "        # Initialize early stopping object\n",
    "        if self.validation_earlystop:\n",
    "            early_stopping = EarlyStopping(patience = self.configNBeats[\"patience\"], verbose = True)\n",
    "        cosine = 0\n",
    "        torch.autograd.set_detect_anomaly(True)\n",
    "        for epoch in range(1, self.configNBeats[\"epochs\"]+1):\n",
    "            \n",
    "            start_time = time()\n",
    "            # Shuffle train data\n",
    "            np.random.shuffle(ts_train_pad)\n",
    "            # Determine number of batches per epoch\n",
    "            num_batches = int(ts_train_pad.shape[0] / self.configNBeats[\"batch_size\"])\n",
    "            self.mylambda = self.configNBeats[\"lambda\"]\n",
    "            \n",
    "            print(epoch,self.mylambda, self.configNBeats[\"lambda\"])    \n",
    "            ####    \n",
    "            \n",
    "            \n",
    "            # Training per epoch\n",
    "            avg_tloss_combined_epoch = 0.0\n",
    "            avg_tloss_forecast_accuracy_epoch = 0.0\n",
    "            avg_tloss_forecast_stability_epoch = 0.0\n",
    "            \n",
    "            for k in range(num_batches):\n",
    "            \n",
    "                batch = np.array(ts_train_pad[k*self.configNBeats[\"batch_size\"]:(k+1)*self.configNBeats[\"batch_size\"]])\n",
    "                x_arr, target_arr = self.make_batch(batch, shuffle_origin = True)\n",
    "                \n",
    "                self.optim.zero_grad() #Set gradients to zero\n",
    "                losses_batch = self.evaluate(x_arr, target_arr,\n",
    "                                             epoch, need_grad = True, \n",
    "                                             early_stop = False)\n",
    "\n",
    "                \n",
    "                \n",
    "                \n",
    "                if self.configNBeats[\"balance_type\"] == \"gcossim\":\n",
    "                    print(torch.cuda.memory_reserved(device=\"cuda:0\"))\n",
    "                    \n",
    "                    \n",
    "                    print(\"gcosim begonnen\")\n",
    "                    #for name, param in self.model.named_parameters():\n",
    "                        #if param.requires_grad:\n",
    "                            #print(name, param.data)\n",
    "                    main_loss = losses_batch[\"forecast_accuracy\"]\n",
    "                    aux_loss = losses_batch[\"forecast_stability\"]\n",
    "                   \n",
    "                    main_grad = torch.autograd.grad(main_loss,self.model.parameters(),retain_graph=True, allow_unused=True)\n",
    "                    #One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.\n",
    "                    # copy\n",
    "                    my_temp_list = list()\n",
    "                    for g in main_grad:\n",
    "                        if g == None:\n",
    "                            my_temp_list.append(None)\n",
    "                        else:\n",
    "                            my_temp_list.append(g.clone())\n",
    "                    #grad = tuple(my_temp_list)\n",
    "                    \n",
    "                    grad = tuple(g for g in my_temp_list)\n",
    "                    aux_grad = torch.autograd.grad(aux_loss, self.model.parameters(),retain_graph=True, allow_unused=True)\n",
    "\n",
    "                    my_temp_list = list()\n",
    "                    for g in main_grad: \n",
    "                        if g != None:\n",
    "                            my_temp_list.append(g)\n",
    "                    \n",
    "                    main_grad_flat = torch.cat(tuple(g.reshape(-1, ) for i, g in enumerate(my_temp_list)), axis=0)\n",
    "                    \n",
    "                    my_temp_list = list()\n",
    "                    for g in aux_grad:\n",
    "                        if g != None:\n",
    "                            my_temp_list.append(g)\n",
    "                    \n",
    "                    aux_grad_flat = torch.cat(tuple(g.reshape(-1, ) for i, g in enumerate(my_temp_list)), axis=0)\n",
    "                    \n",
    "                    cosine = torch.clamp(nn.CosineSimilarity(dim=0)(main_grad_flat, aux_grad_flat), -1, 1)\n",
    "                    print(cosine, \"cosine\")\n",
    "                    \n",
    "                    if cosine > 0: \n",
    "                        my_temp_list = list()\n",
    "                        for g, ga in zip(grad,aux_grad):\n",
    "                            if g != None:\n",
    "                                my_temp_list.append(g+ga)\n",
    "                                #my_temp_list.append(g+ga*cosine) #--> weighted version!\n",
    "                            else: my_temp_list.append(None)\n",
    "                        grad = tuple(my_grad for my_grad in my_temp_list)\n",
    "\n",
    "                        #grad = tuple(g + ga for g, ga in zip(grad, aux_grad))\n",
    "                    loss_combined = main_loss + aux_loss\n",
    "                    loss_combined.backward()\n",
    "                   \n",
    "                    \n",
    "                    for p, g in zip(self.model.parameters(), grad):\n",
    "                        p.grad = g\n",
    "                        #print(p.grad)\n",
    "                   \n",
    "                    self.optim.step()\n",
    "                    \n",
    "                    self.optim.zero_grad()\n",
    "                    del(main_grad,grad,my_temp_list,aux_grad_flat,main_grad_flat,main_loss,aux_loss) #cosine hier weg mss\n",
    "                    torch.cuda.empty_cache()\n",
    "                    \n",
    "                elif self.configNBeats[\"balance_type\"] == \"rw\":\n",
    "                    torch.autograd.set_detect_anomaly(True)\n",
    "                    #implementation with softmax:\n",
    "                    #losses = torch.tensor([losses_batch[\"forecast_accuracy\"],losses_batch[\"forecast_stability\"]])\n",
    "                    \n",
    "                    #weights = F.softmax(torch.randn(2), dim = -1).to(self.device) #RLW: create random weights\n",
    "                    #print(weights)\n",
    "                    \n",
    "                    #WITHOUT CAP\n",
    "                    #weights = torch.rand(2).to(self.device)\n",
    "                    #weights = weights/weights.sum()\n",
    "                    \n",
    "                    #WITHOUT:\n",
    "                    \n",
    "                    nonswitchable = True\n",
    "                    while nonswitchable:\n",
    "                        weights = torch.rand(2).to(self.device)\n",
    "                        weights = weights/weights.sum()\n",
    "                        print(weights, \"before\")\n",
    "                        if weights[0] > self.configNBeats[\"lambda_cap\"] and weights[0] > self.configNBeats[\"lambda_cap\"]:\n",
    "                            continue\n",
    "                        nonswitchable = False\n",
    "                        \n",
    "                        if weights[1] > self.configNBeats[\"lambda_cap\"]:\n",
    "                                weights[0], weights[1] = weights[1].clone(), weights[0].clone()\n",
    "                        print(weights)\n",
    "                        print(\"iteration done\")\n",
    "                    \n",
    "                    weights_cloned = weights.clone()\n",
    "                    #loss = torch.mul(losses, weights_cloned).sum()\n",
    "                    loss_combined = losses_batch[\"forecast_accuracy\"]*weights_cloned[0] + losses_batch[\"forecast_stability\"]*weights_cloned[1]\n",
    "                    self.optim.zero_grad()\n",
    "                    loss_combined.backward()\n",
    "                    self.optim.step()\n",
    "                    self.mylambda = weights[1]/(weights[0] + weights[1])\n",
    "                    \n",
    "\n",
    "                \n",
    "                elif self.configNBeats[\"balance_type\"] == \"gradnorm\":\n",
    "                    #torch.autograd.set_detect_anomaly(True)\n",
    "                    print(torch.cuda.memory_reserved(device=\"cuda:0\"))\n",
    "                    losses_batch_tensor = torch.tensor([losses_batch[\"forecast_accuracy\"],losses_batch[\"forecast_stability\"]], device= self.device) #create tensor object\n",
    "                    \n",
    "                    if epoch == 1 and k ==0:\n",
    "                        balance_weights = torch.ones(2, device = self.device) #weights used to balance tasks: weight1*accuracy + weight2*instability\n",
    "                        balance_weights[0] = 1.9\n",
    "                        balance_weights[1] = 0.1 #start weights (can be adjusted to get better results)\n",
    "                        balance_weights = torch.nn.Parameter(balance_weights) #So gradient can be calculated \n",
    "                        T = 2 # 2 tasks\n",
    "                        self.optim_grad = torch.optim.Adam([balance_weights],lr = self.configNBeats[\"learning_rate_gradnorm\"]) #create second optimizer\n",
    "                        l0 = losses_batch_tensor.detach() #loss in first iteration\n",
    "                        #get right layer:\n",
    "                        myblock = self.model.stacks[self.configNBeats[\"n_stacks\"]-1][self.configNBeats[\"nb_blocks_per_stack\"]-1] #last blcok\n",
    "                        lastlayer = myblock.fc4 #(last shared layer of last block of laatste stack\n",
    "                    balance_weights_cloned = balance_weights.clone()\n",
    "                    loss_combined = (balance_weights_cloned[0]*losses_batch[\"forecast_accuracy\"]+balance_weights_cloned[1]*losses_batch[\"forecast_stability\"])\n",
    "                    #loss_combined =  balance_weights @ losses_batch_tensor #matrix multiplication \n",
    "                    #backward pass for weighted task loss:\n",
    "                    loss_combined.backward(retain_graph=True) \n",
    "                    gw = []\n",
    "\n",
    "                    #Calculate the two task gradients with respect to the last shared layer\n",
    "                    for i in range(0,len(losses_batch_tensor)):  \n",
    "                        if i ==0:\n",
    "                            my_temp_loss = \"forecast_accuracy\" \n",
    "                        else:\n",
    "                            my_temp_loss =\"forecast_stability\"\n",
    "                       \n",
    "                        #task_gradient =  torch.autograd.grad(balance_weights[i]*losses_batch[my_temp_loss], self.model.parameters(), allow_unused = True,retain_graph=True, create_graph=True)[0] \n",
    "\n",
    "                        task_gradient =  torch.autograd.grad(balance_weights[i]*losses_batch[my_temp_loss], lastlayer.parameters(), retain_graph=True, create_graph=True)[0] \n",
    "                        #task_gradient =  torch.autograd.grad(balance_weights[i]*self.evaluate(x_arr, target_arr,\n",
    "                        #                     epoch, need_grad = True, \n",
    "                        #                    early_stop = False)[my_temp_loss], lastlayer.parameters(), retain_graph=True, create_graph=True,)[0] #calculate gradients with respect to last layer\n",
    "                        #print(task_gradient)\n",
    "                        gw.append(torch.linalg.norm(task_gradient)) #take L2 norm for each task\n",
    "                    print(gw)\n",
    "                    #See definitions GradNorm paper\n",
    "                    gw = torch.stack(gw) #make it a tensor\n",
    "                    loss_ratio = losses_batch_tensor.detach() /l0 \n",
    "                    avg_gw = gw.mean().detach() \n",
    "                    rt = loss_ratio/loss_ratio.mean() \n",
    "                    desired_grad = (avg_gw*rt**self.configNBeats[\"alpha\"]).detach()\n",
    "                    lgrad = torch.abs(gw-desired_grad).sum()\n",
    "                    #clear gradients for the gradnorm optimizer:\n",
    "                    self.optim_grad.zero_grad()\n",
    "                    #calculate gradients + do optimization step on weights!\n",
    "                    lgrad.backward()\n",
    "                    self.optim_grad.step()\n",
    "                    #backward pass for weighted task loss:\n",
    "                    #self.optim.zero_grad()\n",
    "                    #Update weights in the network using the combined loss!\n",
    "                    #print(\"loss_combined\")\n",
    "                    #loss_combined.backward(retain_graph = True) \n",
    "                    #print(\"backward klaar\")\n",
    "                    self.optim.step()\n",
    "                    balance_weights = (balance_weights/balance_weights.sum()*T).detach()\n",
    "                    print(balance_weights)\n",
    "                    balance_weights = torch.nn.Parameter(balance_weights)\n",
    "                    self.optim_grad = torch.optim.Adam([balance_weights], lr= self.configNBeats[\"learning_rate_gradnorm\"])\n",
    "                    self.mylambda = balance_weights[1]/(balance_weights.sum())\n",
    "                    del(gw,loss_ratio,rt,desired_grad,lgrad,task_gradient)\n",
    "                    torch.cuda.empty_cache()\n",
    "              \n",
    "                else:    \n",
    "                    \n",
    "                    if self.shifts > 0:\n",
    "                        #loss_combined = ((self.configNBeats[\"lambda\"] * losses_batch[\"forecast_stability\"]) +\n",
    "                        #((1 - self.configNBeats[\"lambda\"]) * losses_batch[\"forecast_accuracy\"]))\n",
    "                        loss_combined = ((self.mylambda * losses_batch[\"forecast_stability\"]) +\n",
    "                        ((1 - self.mylambda) * losses_batch[\"forecast_accuracy\"]))\n",
    "                    else:\n",
    "                        loss_combined = losses_batch[\"forecast_accuracy\"]\n",
    "                \n",
    "                \n",
    "                    loss_combined.backward() #calculates gradients based on loss\n",
    "                    self.optim.step() #optimizer.step is performs a parameter update based on the current gradient \n",
    "                    #(stored in .grad attribute of a parameter) and the update rule \n",
    "                    #(https://discuss.pytorch.org/t/how-are-optimizer-step-and-loss-backward-related/7350)\n",
    "                \n",
    "                    #params = self.model.parameters()\n",
    "                    #total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)\n",
    "                    #if (epoch == 1 or epoch == self.configNBeats[\"epochs\"]) and k == 0:\n",
    "                    #    print('Epoch {}/{} \\t n_learnable_pars={:.4f}'.format(\n",
    "                    #        epoch,\n",
    "                    #        self.configNBeats[\"epochs\"],\n",
    "                    #        total_params))\n",
    "                \n",
    "                wandb.log({\"tloss_comb_step\": loss_combined,\n",
    "                        \"tloss_fcacc_step\": losses_batch[\"forecast_accuracy\"],\n",
    "                        \"tloss_fcstab_step\": losses_batch[\"forecast_stability\"],\n",
    "                          \"cosine\" : cosine,\n",
    "                          \"lambda\": self.mylambda})\n",
    "                #print(\"wandb logt\")\n",
    "                avg_tloss_combined_epoch += (loss_combined / num_batches)\n",
    "                avg_tloss_forecast_accuracy_epoch += (losses_batch[\"forecast_accuracy\"] / num_batches)\n",
    "                avg_tloss_forecast_stability_epoch += (losses_batch[\"forecast_stability\"] / num_batches)\n",
    "                \n",
    "            if self.validation: # validation_full and validation_earlystop\n",
    "               \n",
    "                # Evaluation per epoch\n",
    "                avg_eloss_combined_epoch = 0.0\n",
    "                avg_eloss_forecast_accuracy_epoch = 0.0\n",
    "                avg_eloss_forecast_stability_epoch = 0.0\n",
    "\n",
    "                for forigin in range(self.forigins):\n",
    "                    \n",
    "                    # Only one batch, but one batch per forecast origin\n",
    "                    if forigin < self.forigins-1:\n",
    "                        eval_data_subset = np.array([x[:(-18 + forigin + self.forecast_length)] for x in ts_eval_pad],\n",
    "                                                   dtype = object)\n",
    "                    else:\n",
    "                        eval_data_subset = np.array([x for x in ts_eval_pad], dtype = object)\n",
    "                    x_arr, target_arr = self.make_batch(eval_data_subset, shuffle_origin = False)\n",
    "                    \n",
    "                    losses_evaluation = self.evaluate(x_arr, target_arr,\n",
    "                                                      epoch, need_grad = False,\n",
    "                                                      early_stop = False)\n",
    "                    \n",
    "                    if self.shifts > 0:\n",
    "                        #loss_combined = ((self.configNBeats[\"lambda\"] * losses_evaluation[\"forecast_stability\"]) +\n",
    "                        #                 ((1 - self.configNBeats[\"lambda\"]) * losses_evaluation[\"forecast_accuracy\"]))\n",
    "                        loss_combined = ((self.mylambda * losses_evaluation[\"forecast_stability\"]) +\n",
    "                                         ((1 - self.mylambda) * losses_evaluation[\"forecast_accuracy\"]))\n",
    "                    else:\n",
    "                        loss_combined = losses_evaluation[\"forecast_accuracy\"]\n",
    "\n",
    "                    avg_eloss_combined_epoch += (loss_combined / self.forigins)\n",
    "                    avg_eloss_forecast_accuracy_epoch += (losses_evaluation[\"forecast_accuracy\"] / self.forigins)\n",
    "                    avg_eloss_forecast_stability_epoch += (losses_evaluation[\"forecast_stability\"] / self.forigins)\n",
    "                \n",
    "                elapsed_time = time() - start_time\n",
    "\n",
    "                print('Epoch {}/{} \\t tloss_combined={:.4f} \\t eloss_combined={:.4f} \\t time={:.2f}s \\t lambda={:.2f}'.format(\n",
    "                    epoch,\n",
    "                    self.configNBeats[\"epochs\"],\n",
    "                    avg_tloss_combined_epoch,\n",
    "                    avg_eloss_combined_epoch,\n",
    "                    elapsed_time,\n",
    "                    self.mylambda))\n",
    "                \n",
    "                wandb.log({\"epoch\": epoch,\n",
    "                           \"tloss_comb_evol\": avg_tloss_combined_epoch,\n",
    "                           \"tloss_fcacc_evol\": avg_tloss_forecast_accuracy_epoch,\n",
    "                           \"tloss_fcstab_evol\": avg_tloss_forecast_stability_epoch,\n",
    "                           \"eloss_comb_evol\": avg_eloss_combined_epoch,\n",
    "                           \"eloss_fcacc_evol\": avg_eloss_forecast_accuracy_epoch,\n",
    "                           \"eloss_fcstab_evol\": avg_eloss_forecast_stability_epoch})\n",
    "                \n",
    "                if self.validation_earlystop: # validation_earlystop \n",
    "\n",
    "                    # early_stopping needs the average epoch validation loss to check if it has decreased, \n",
    "                    # and if it has, it will make a checkpoint of the current model\n",
    "                    early_stopping(avg_eloss_combined_epoch, self.model)\n",
    "\n",
    "                    if early_stopping.early_stop:\n",
    "                        print(\"Early stopping\")\n",
    "                        # Load the last checkpoint with the best model\n",
    "                        self.model.load_state_dict(torch.load('checkpoint.pt'))\n",
    "                        # Produce plots for final epoch before break\n",
    "                        if not self.disable_plot:\n",
    "                            for forigin in range(self.forigins):\n",
    "                                # Only one batch, but one batch per forecast origin\n",
    "                                if forigin < self.forigins-1:\n",
    "                                    eval_data_subset = np.array([x[:(-18 + forigin + self.forecast_length)] for x in ts_eval_pad],\n",
    "                                                                dtype = object)\n",
    "                                else:\n",
    "                                    eval_data_subset = np.array([x for x in ts_eval_pad], dtype = object)\n",
    "                                x_arr, target_arr = self.make_batch(eval_data_subset, shuffle_origin = False)\n",
    "\n",
    "                                losses_evaluation = self.evaluate(x_arr, target_arr,\n",
    "                                                                  epoch, need_grad = False,\n",
    "                                                                  early_stop = True)\n",
    "                        # Break loop over epochs\n",
    "                        break\n",
    "                    \n",
    "            else: # testing\n",
    "                \n",
    "                elapsed_time = time() - start_time\n",
    "\n",
    "                print('Epoch {}/{} \\t tloss_combined={:.4f} \\t time={:.2f}s'.format(\n",
    "                    epoch,\n",
    "                    self.configNBeats[\"epochs\"],\n",
    "                    avg_tloss_combined_epoch,\n",
    "                    elapsed_time))\n",
    "                \n",
    "                wandb.log({\"epoch\": epoch,\n",
    "                           \"tloss_comb_evol\": avg_tloss_combined_epoch,\n",
    "                           \"tloss_fcacc_evol\": avg_tloss_forecast_accuracy_epoch,\n",
    "                           \"tloss_fcstab_evol\": avg_tloss_forecast_stability_epoch})\n",
    "\n",
    "        wandb.log({\"tloss_comb\": avg_tloss_combined_epoch,\n",
    "                   \"tloss_fcacc\": avg_tloss_forecast_accuracy_epoch,\n",
    "                   \"tloss_fcstab\": avg_tloss_forecast_stability_epoch})\n",
    "        \n",
    "        print('--- Training done ---')\n",
    "        print('--- Final evaluation ---')\n",
    "        \n",
    "        print('--- M4 evaluation ---')\n",
    "        \n",
    "        # Containers to save actuals and forecasts\n",
    "        actuals = np.empty(shape = (len(ts_eval_pad), self.forigins, self.forecast_length)) # n_series, forigin, forecast_length\n",
    "        forecasts = np.empty(shape = (len(ts_eval_pad), self.forigins, self.forecast_length)) # n_series, forigin, forecast_length\n",
    "        \n",
    "        # Forecasts for each origin in rolling_window\n",
    "        for forigin in range(self.forigins):\n",
    "            \n",
    "            # Only one batch, but one batch per forecast origin\n",
    "            if forigin < self.forigins-1:\n",
    "                eval_data_subset = np.array([x[:(-18 + forigin + self.forecast_length)] for x in ts_eval_pad],\n",
    "                                           dtype = object)\n",
    "            else:\n",
    "                eval_data_subset = np.array([x for x in ts_eval_pad], dtype = object)\n",
    "            x_arr, target_arr = self.make_batch(eval_data_subset, shuffle_origin = False)\n",
    "            \n",
    "            # Produce forecasts for subset of test data\n",
    "            x_arr = torch.from_numpy(x_arr).float().to(self.device)\n",
    "            target_arr = torch.from_numpy(target_arr).float().to(self.device)\n",
    "            with torch.no_grad():\n",
    "                self.model.eval()\n",
    "                self.model.to(self.device)\n",
    "                _, forecast_arr = self.model(x_arr)\n",
    "                \n",
    "            x_arr = x_arr.cpu() \n",
    "            target_arr = target_arr.cpu()\n",
    "            forecast_arr = forecast_arr.cpu()\n",
    "                \n",
    "            # Plot 10 random examples per origin - of standard/unshifted input\n",
    "            sample_ids = np.random.randint(low = 0, high = int(x_arr.shape[0]), size = 10)\n",
    "            for sample_id in sample_ids:\n",
    "                self.create_example_plots(forecast_arr[sample_id, 0, :], \n",
    "                                          target_arr[sample_id, 0, :], \n",
    "                                          x_arr[sample_id, 0, :],\n",
    "                                          final_evaluation = True)\n",
    "                \n",
    "            # Save to containers\n",
    "            forecasts[:, forigin, :] = forecast_arr[:, 0, :]\n",
    "            actuals[:, forigin, :] = target_arr[:, 0, :]\n",
    "            \n",
    "        # Compute accuracy sMAPE\n",
    "        sMAPE = 200 * np.mean(np.abs(actuals - forecasts) / (np.abs(forecasts) + np.abs(actuals)))\n",
    "        \n",
    "        # Compute stability Total MAC\n",
    "        if self.forecast_length == 6:\n",
    "            weight = np.mean(actuals[:, [0, 6, 12], :], axis = (1, 2))\n",
    "        elif self.forecast_length == 18:\n",
    "            weight = np.mean(actuals[:, 0, :], axis = -1)\n",
    "        forecasts_helper = np.full((actuals.shape[0], \n",
    "                                    self.forigins,\n",
    "                                    (self.forecast_length - 1) + self.forigins), np.nan)\n",
    "        # n_series x self.forigins x ((forecast_length - 1) + forigins)\n",
    "        for forigin in range(self.forigins):\n",
    "            forecasts_helper[:, forigin, forigin:(forigin + self.forecast_length)] = forecasts[:, forigin, :]\n",
    "        MAC_mat = np.abs(np.diff(forecasts_helper, axis = 1))\n",
    "        # n_series x (self.forigins - 1) x ((forecast_length - 1) + forigins)\n",
    "        MAC_mat_adjust = np.delete(MAC_mat, [0, (self.forecast_length - 1) + self.forigins - 1], 2)\n",
    "        # n_series x (self.forigins - 1) x ((forecast_length - 1) + forigins - 2)\n",
    "        MAC = np.nanmean(MAC_mat_adjust, axis = 1)\n",
    "        # n_series x ((forecast_length - 1) + forigins - 2)\n",
    "        ItemMAC = np.mean(MAC, axis = 1) / weight\n",
    "        TotalMAC = np.mean(ItemMAC) * 100\n",
    "        \n",
    "        print('sMAPE_m4m={:.4f} \\t TotalMAC_m4m={:.4f}'.format(sMAPE, TotalMAC))\n",
    "        \n",
    "        wandb.log({\"sMAPE_m4m\": sMAPE,\n",
    "                   \"TotalMAC_m4m\": TotalMAC})\n",
    "        \n",
    "        # n_series, forigin, forecast_length\n",
    "        fc_colnames = [str(i) for i in range(1, self.forecast_length + 1)]\n",
    "        \n",
    "        actuals_np = actuals#.numpy()\n",
    "        m,n,r = actuals_np.shape\n",
    "        actuals_arr = np.column_stack((np.repeat(np.arange(m) + 1, n), \n",
    "                                       np.tile(np.arange(n) + 1, m),\n",
    "                                       actuals_np.reshape(m*n, -1)))\n",
    "        actuals_df = pd.DataFrame(actuals_arr, columns = ['item_id', 'fc_origin'] + fc_colnames)\n",
    "        helper_col = ['actual'] * len(actuals_df)\n",
    "        actuals_df['type'] = helper_col\n",
    "        \n",
    "        forecasts_np = forecasts#.numpy()\n",
    "        m,n,r = forecasts_np.shape\n",
    "        forecasts_arr = np.column_stack((np.repeat(np.arange(m) + 1, n), \n",
    "                                         np.tile(np.arange(n) + 1, m),\n",
    "                                         forecasts_np.reshape(m*n, -1)))\n",
    "        forecasts_df = pd.DataFrame(forecasts_arr, columns = ['item_id', 'fc_origin'] + fc_colnames)\n",
    "        helper_col = ['forecast'] * len(forecasts_df)\n",
    "        forecasts_df['type'] = helper_col\n",
    "        \n",
    "        output_df_m4m = pd.concat([actuals_df, forecasts_df])\n",
    "         \n",
    "        wandb.join()\n",
    "        \n",
    "        return output_df_m4m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb_project_name = 'nbeats_stability_m4_monthly'\n",
    "job_type_name = 'validation_full' \n",
    "\n",
    "# one of:\n",
    "# - 'test', \n",
    "# - 'validation_full' --> e.g., for lambda value tuning\n",
    "# - 'validation_earlystop' --> for specifying number of epochs\n",
    "#forigins wordt gedefinieerd hieronder (13)\n",
    "\n",
    "\n",
    "hyperparameter_defaults = dict()\n",
    "hyperparameter_defaults['epochs'] = 155  #M4: 155, M3 4000\n",
    "hyperparameter_defaults['batch_size'] = 512 #512 \n",
    "hyperparameter_defaults['nb_blocks_per_stack'] = 1\n",
    "hyperparameter_defaults['thetas_dims'] = 256 #256 \n",
    "hyperparameter_defaults['n_stacks'] = 20  #20 \n",
    "hyperparameter_defaults['share_weights_in_stack'] = False\n",
    "hyperparameter_defaults[\"backcast_length_multiplier\"] = 6#M3: 6, M4: 4\n",
    "hyperparameter_defaults['hidden_layer_units'] = 256 #256\n",
    "hyperparameter_defaults['share_thetas'] = False\n",
    "hyperparameter_defaults[\"dropout\"] = False\n",
    "hyperparameter_defaults[\"dropout_p\"] = 0.0\n",
    "hyperparameter_defaults[\"neg_slope\"] = 0.00\n",
    "hyperparameter_defaults['learning_rate'] = 0.00001 #0.001 in M4, 0.00001 in M3\n",
    "hyperparameter_defaults[\"weight_decay\"] = 0.00\n",
    "hyperparameter_defaults[\"LH\"] = 20 #10 in M4 #20 in M3\n",
    "hyperparameter_defaults[\"rndseed\"] = 2000\n",
    "hyperparameter_defaults[\"loss_function\"] = 1 # 1 == RMSSE / 2 == RMSSE_m / 3 == SMAPE / 4 == MAPE\n",
    "hyperparameter_defaults[\"shifts\"] = 1\n",
    "hyperparameter_defaults['patience'] = 2000 # Only affects 'validation_earlystop' runs\n",
    "hyperparameter_defaults[\"lambda\"] = 0.46\n",
    "hyperparameter_defaults[\"balance_type\"] = \"rw\" #\"gradnorm\" or \"no\" or \"gcossim\" or \"rw\" #manually change line of code in\n",
    "#train_net to get weighted/unweighted gcosim, regular rw can be achie\n",
    "hyperparameter_defaults[\"alpha\"] = 1 #hyperparameter voor gradnorm\n",
    "hyperparameter_defaults[\"learning_rate_gradnorm\"] = 0.0025 #hyperparameter voor gradnorm \n",
    "hyperparameter_defaults[\"lambda_cap\"] = 0.5\n",
    "#Om een of andere reden moet dit ook in die sweep worden aangepast??\n",
    "#0.15 # Weight of forecast stability loss in loss_combined\n",
    "# Note that lambda is defined in the code as the proportion of stability in total loss (relative terms)\n",
    "# In the paper, lambda is defined in absolute terms\n",
    "# So: lambda_paper = lambda_code/(1-lambda_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "if job_type_name == 'test':\n",
    "    is_val = False\n",
    "    do_earlystop = False\n",
    "    m4_train, m4_eval = valset_m4m, testset_m4m\n",
    "elif job_type_name == 'validation_full':\n",
    "    is_val = True\n",
    "    do_earlystop = False\n",
    "    m4_train, m4_eval = trainset_m4m, valset_m4m\n",
    "elif job_type_name == 'validation_earlystop':\n",
    "    is_val = True\n",
    "    do_earlystop = True\n",
    "    m4_train, m4_eval = trainset_m4m, valset_m4m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sweep_function():\n",
    "    wandb.init(config = hyperparameter_defaults,\n",
    "               project = wandb_project_name,\n",
    "               job_type = job_type_name)\n",
    "    config = wandb.config\n",
    "    run_name = wandb.run.name\n",
    "    \n",
    "    # Initialize model\n",
    "    StableNBeats_model = StableNBeatsLearner(device, 6, config) #length of forecast\n",
    "\n",
    "    # Train & evaluate\n",
    "    forecasts_df_m4m = StableNBeats_model.train_net(m4_train, m4_eval, 13, is_val, do_earlystop) #13 is forigins\n",
    "    # Save forecasts\n",
    "    forecasts_df_m4m.to_csv('m4m_nbeats_stability_' + job_type_name + '_' + run_name + '.csv', index = False)\n",
    "    #forecasts_df_m4m.to_csv('/content/drive/My Drive/Colab Notebooks/Sweeps/m4m_nbeats_stability_' + job_type_name + '_' + run_name + '.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: nx7v082f\n",
      "Sweep URL: https://wandb.ai/n-beats-s-thesis/nbeats_stability_m4_monthly/sweeps/nx7v082f\n"
     ]
    }
   ],
   "source": [
    "sweep_config = {\n",
    "    \"name\": \"sweep\",\n",
    "    \"method\": \"grid\",\n",
    "    \"parameters\": {\n",
    "        \"rndseed\": {\n",
    "            \"values\": [2000] #verander naar 2000\n",
    "        },\n",
    "        \"weight_decay\": {\n",
    "            \"values\": [0.0]\n",
    "        },\n",
    "        \"dropout_p\": {\n",
    "            \"values\": [0.0]\n",
    "        },\n",
    "         \"alpha\": {\n",
    "            \"values\": [0]\n",
    "        \n",
    "        },\n",
    "         \"learning_rate_gradnorm\": {\n",
    "            \"values\": [0.0025]\n",
    "        },\n",
    "        \"lambda_cap\": {\n",
    "            \"values\": [0.3]\n",
    "        },\n",
    "        \"balance_type\": {\n",
    "            \"values\": [\"rw\"]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "sweep_id = wandb.sweep(sweep_config, project = wandb_project_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: mo1n6zt6 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbalance_type: rw\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout_p: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlambda_cap: 0.3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate_gradnorm: 0.0025\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trndseed: 2000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.14.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\DaanC\\Downloads\\wandb\\run-20230418_195702-mo1n6zt6</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/n-beats-s-thesis/nbeats_stability_m4_monthly/runs/mo1n6zt6' target=\"_blank\">peachy-sweep-1</a></strong> to <a href='https://wandb.ai/n-beats-s-thesis/nbeats_stability_m4_monthly' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/n-beats-s-thesis/nbeats_stability_m4_monthly/sweeps/nx7v082f' target=\"_blank\">https://wandb.ai/n-beats-s-thesis/nbeats_stability_m4_monthly/sweeps/nx7v082f</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/n-beats-s-thesis/nbeats_stability_m4_monthly' target=\"_blank\">https://wandb.ai/n-beats-s-thesis/nbeats_stability_m4_monthly</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/n-beats-s-thesis/nbeats_stability_m4_monthly/sweeps/nx7v082f' target=\"_blank\">https://wandb.ai/n-beats-s-thesis/nbeats_stability_m4_monthly/sweeps/nx7v082f</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/n-beats-s-thesis/nbeats_stability_m4_monthly/runs/mo1n6zt6' target=\"_blank\">https://wandb.ai/n-beats-s-thesis/nbeats_stability_m4_monthly/runs/mo1n6zt6</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Model ---\n",
      "| N-Beats\n",
      "| --  Stack Generic (#0) (share_weights_in_stack=False)\n",
      "     | -- GenericNBeatsBlock(units=[256, 256, 256, 256], thetas_dims=256, backcast_length=36, forecast_length=6, share_thetas=False, dropout=False, dropout_p=0, neg_slope=0.0) at @2163342706144\n",
      "| --  Stack Generic (#1) (share_weights_in_stack=False)\n",
      "     | -- GenericNBeatsBlock(units=[256, 256, 256, 256], thetas_dims=256, backcast_length=36, forecast_length=6, share_thetas=False, dropout=False, dropout_p=0, neg_slope=0.0) at @2163342705424\n",
      "| --  Stack Generic (#2) (share_weights_in_stack=False)\n",
      "     | -- GenericNBeatsBlock(units=[256, 256, 256, 256], thetas_dims=256, backcast_length=36, forecast_length=6, share_thetas=False, dropout=False, dropout_p=0, neg_slope=0.0) at @2163342708304\n",
      "| --  Stack Generic (#3) (share_weights_in_stack=False)\n",
      "     | -- GenericNBeatsBlock(units=[256, 256, 256, 256], thetas_dims=256, backcast_length=36, forecast_length=6, share_thetas=False, dropout=False, dropout_p=0, neg_slope=0.0) at @2161154935152\n",
      "| --  Stack Generic (#4) (share_weights_in_stack=False)\n",
      "     | -- GenericNBeatsBlock(units=[256, 256, 256, 256], thetas_dims=256, backcast_length=36, forecast_length=6, share_thetas=False, dropout=False, dropout_p=0, neg_slope=0.0) at @2161154935440\n",
      "| --  Stack Generic (#5) (share_weights_in_stack=False)\n",
      "     | -- GenericNBeatsBlock(units=[256, 256, 256, 256], thetas_dims=256, backcast_length=36, forecast_length=6, share_thetas=False, dropout=False, dropout_p=0, neg_slope=0.0) at @2161154933856\n",
      "| --  Stack Generic (#6) (share_weights_in_stack=False)\n",
      "     | -- GenericNBeatsBlock(units=[256, 256, 256, 256], thetas_dims=256, backcast_length=36, forecast_length=6, share_thetas=False, dropout=False, dropout_p=0, neg_slope=0.0) at @2162252932144\n",
      "| --  Stack Generic (#7) (share_weights_in_stack=False)\n",
      "     | -- GenericNBeatsBlock(units=[256, 256, 256, 256], thetas_dims=256, backcast_length=36, forecast_length=6, share_thetas=False, dropout=False, dropout_p=0, neg_slope=0.0) at @2162252796016\n",
      "| --  Stack Generic (#8) (share_weights_in_stack=False)\n",
      "     | -- GenericNBeatsBlock(units=[256, 256, 256, 256], thetas_dims=256, backcast_length=36, forecast_length=6, share_thetas=False, dropout=False, dropout_p=0, neg_slope=0.0) at @2162252798320\n",
      "| --  Stack Generic (#9) (share_weights_in_stack=False)\n",
      "     | -- GenericNBeatsBlock(units=[256, 256, 256, 256], thetas_dims=256, backcast_length=36, forecast_length=6, share_thetas=False, dropout=False, dropout_p=0, neg_slope=0.0) at @2162252799472\n",
      "| --  Stack Generic (#10) (share_weights_in_stack=False)\n",
      "     | -- GenericNBeatsBlock(units=[256, 256, 256, 256], thetas_dims=256, backcast_length=36, forecast_length=6, share_thetas=False, dropout=False, dropout_p=0, neg_slope=0.0) at @2162252801888\n",
      "| --  Stack Generic (#11) (share_weights_in_stack=False)\n",
      "     | -- GenericNBeatsBlock(units=[256, 256, 256, 256], thetas_dims=256, backcast_length=36, forecast_length=6, share_thetas=False, dropout=False, dropout_p=0, neg_slope=0.0) at @2162252947952\n",
      "| --  Stack Generic (#12) (share_weights_in_stack=False)\n",
      "     | -- GenericNBeatsBlock(units=[256, 256, 256, 256], thetas_dims=256, backcast_length=36, forecast_length=6, share_thetas=False, dropout=False, dropout_p=0, neg_slope=0.0) at @2162879625392\n",
      "| --  Stack Generic (#13) (share_weights_in_stack=False)\n",
      "     | -- GenericNBeatsBlock(units=[256, 256, 256, 256], thetas_dims=256, backcast_length=36, forecast_length=6, share_thetas=False, dropout=False, dropout_p=0, neg_slope=0.0) at @2162879626640\n",
      "| --  Stack Generic (#14) (share_weights_in_stack=False)\n",
      "     | -- GenericNBeatsBlock(units=[256, 256, 256, 256], thetas_dims=256, backcast_length=36, forecast_length=6, share_thetas=False, dropout=False, dropout_p=0, neg_slope=0.0) at @2162400868240\n",
      "| --  Stack Generic (#15) (share_weights_in_stack=False)\n",
      "     | -- GenericNBeatsBlock(units=[256, 256, 256, 256], thetas_dims=256, backcast_length=36, forecast_length=6, share_thetas=False, dropout=False, dropout_p=0, neg_slope=0.0) at @2162400866560\n",
      "| --  Stack Generic (#16) (share_weights_in_stack=False)\n",
      "     | -- GenericNBeatsBlock(units=[256, 256, 256, 256], thetas_dims=256, backcast_length=36, forecast_length=6, share_thetas=False, dropout=False, dropout_p=0, neg_slope=0.0) at @2162400869632\n",
      "| --  Stack Generic (#17) (share_weights_in_stack=False)\n",
      "     | -- GenericNBeatsBlock(units=[256, 256, 256, 256], thetas_dims=256, backcast_length=36, forecast_length=6, share_thetas=False, dropout=False, dropout_p=0, neg_slope=0.0) at @2162400868336\n",
      "| --  Stack Generic (#18) (share_weights_in_stack=False)\n",
      "     | -- GenericNBeatsBlock(units=[256, 256, 256, 256], thetas_dims=256, backcast_length=36, forecast_length=6, share_thetas=False, dropout=False, dropout_p=0, neg_slope=0.0) at @2162400868192\n",
      "| --  Stack Generic (#19) (share_weights_in_stack=False)\n",
      "     | -- GenericNBeatsBlock(units=[256, 256, 256, 256], thetas_dims=256, backcast_length=36, forecast_length=6, share_thetas=False, dropout=False, dropout_p=0, neg_slope=0.0) at @2163341024992\n",
      "--- Training ---\n",
      "1 0.46 0.46\n",
      "tensor([0.6712, 0.3288], device='cuda:0') before\n",
      "tensor([0.9105, 0.0895], device='cuda:0') before\n",
      "tensor([0.1601, 0.8399], device='cuda:0') before\n",
      "tensor([0.8399, 0.1601], device='cuda:0')\n",
      "iteratie klaar\n",
      "tensor([0.1610, 0.8390], device='cuda:0') before\n",
      "tensor([0.8390, 0.1610], device='cuda:0')\n",
      "iteratie klaar\n",
      "nu begint validation\n",
      "Epoch 1/3 \t tloss_combined=3.6237 \t eloss_combined=3.6757 \t time=3.11s \t lambda=0.16\n",
      "2 0.46 0.46\n",
      "tensor([0.3184, 0.6816], device='cuda:0') before\n",
      "tensor([0.2261, 0.7739], device='cuda:0') before\n",
      "tensor([0.7739, 0.2261], device='cuda:0')\n",
      "iteratie klaar\n",
      "tensor([0.3152, 0.6848], device='cuda:0') before\n",
      "tensor([0.4295, 0.5705], device='cuda:0') before\n",
      "tensor([0.6520, 0.3480], device='cuda:0') before\n",
      "tensor([0.1937, 0.8063], device='cuda:0') before\n",
      "tensor([0.8063, 0.1937], device='cuda:0')\n",
      "iteratie klaar\n",
      "nu begint validation\n",
      "Epoch 2/3 \t tloss_combined=3.3811 \t eloss_combined=3.5034 \t time=2.52s \t lambda=0.19\n",
      "3 0.46 0.46\n",
      "tensor([0.6601, 0.3399], device='cuda:0') before\n",
      "tensor([0.3998, 0.6002], device='cuda:0') before\n",
      "tensor([0.6445, 0.3555], device='cuda:0') before\n",
      "tensor([0.4677, 0.5323], device='cuda:0') before\n",
      "tensor([0.5586, 0.4414], device='cuda:0') before\n",
      "tensor([0.1693, 0.8307], device='cuda:0') before\n",
      "tensor([0.8307, 0.1693], device='cuda:0')\n",
      "iteratie klaar\n",
      "tensor([0.4612, 0.5388], device='cuda:0') before\n",
      "tensor([0.3803, 0.6197], device='cuda:0') before\n",
      "tensor([0.2057, 0.7943], device='cuda:0') before\n",
      "tensor([0.7943, 0.2057], device='cuda:0')\n",
      "iteratie klaar\n",
      "nu begint validation\n",
      "Epoch 3/3 \t tloss_combined=3.4141 \t eloss_combined=3.4211 \t time=2.56s \t lambda=0.21\n",
      "--- Training done ---\n",
      "--- Final evaluation ---\n",
      "--- M4 evaluation ---\n",
      "sMAPE_m4m=191.0259 \t TotalMAC_m4m=0.7266\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8ce62d154eb4f2d846c4b2e4a415812",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.384 MB of 1.104 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=0.347901"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>TotalMAC_m4m</td><td></td></tr><tr><td>cosine</td><td></td></tr><tr><td>eloss_comb_evol</td><td></td></tr><tr><td>eloss_fcacc_evol</td><td></td></tr><tr><td>eloss_fcstab_evol</td><td></td></tr><tr><td>epoch</td><td></td></tr><tr><td>lambda</td><td></td></tr><tr><td>sMAPE_m4m</td><td></td></tr><tr><td>tloss_comb</td><td></td></tr><tr><td>tloss_comb_evol</td><td></td></tr><tr><td>tloss_comb_step</td><td></td></tr><tr><td>tloss_fcacc</td><td></td></tr><tr><td>tloss_fcacc_evol</td><td></td></tr><tr><td>tloss_fcacc_step</td><td></td></tr><tr><td>tloss_fcstab</td><td></td></tr><tr><td>tloss_fcstab_evol</td><td></td></tr><tr><td>tloss_fcstab_step</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>TotalMAC_m4m</td><td>0.72656</td></tr><tr><td>cosine</td><td>0</td></tr><tr><td>eloss_comb_evol</td><td>3.42109</td></tr><tr><td>eloss_fcacc_evol</td><td>4.24384</td></tr><tr><td>eloss_fcstab_evol</td><td>0.24445</td></tr><tr><td>epoch</td><td>3</td></tr><tr><td>lambda</td><td>0.20572</td></tr><tr><td>sMAPE_m4m</td><td>191.02592</td></tr><tr><td>tloss_comb</td><td>3.41406</td></tr><tr><td>tloss_comb_evol</td><td>3.41406</td></tr><tr><td>tloss_comb_step</td><td>3.31586</td></tr><tr><td>tloss_fcacc</td><td>4.12541</td></tr><tr><td>tloss_fcacc_evol</td><td>4.12541</td></tr><tr><td>tloss_fcacc_step</td><td>4.09637</td></tr><tr><td>tloss_fcstab</td><td>0.33177</td></tr><tr><td>tloss_fcstab_evol</td><td>0.33177</td></tr><tr><td>tloss_fcstab_step</td><td>0.30231</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">peachy-sweep-1</strong> at: <a href='https://wandb.ai/n-beats-s-thesis/nbeats_stability_m4_monthly/runs/mo1n6zt6' target=\"_blank\">https://wandb.ai/n-beats-s-thesis/nbeats_stability_m4_monthly/runs/mo1n6zt6</a><br/>Synced 6 W&B file(s), 130 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230418_195702-mo1n6zt6\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Exiting.\n"
     ]
    }
   ],
   "source": [
    "wandb.agent(sweep_id, function = sweep_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "https://medium.com/@bryan.santos/lord-of-the-notebooks-optimizing-jupyter-9cc168debcc7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wandb login --relogin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.login()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "1cc7ae5e145cd8c4187fba1de8cdafb4e484fdd4abb8559737df5352336dc806"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
